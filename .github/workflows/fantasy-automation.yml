name: Daily Fantasy Scrapers

on:
  schedule:
    - cron: '14 14 * * *'  # Runs daily at 9:14 AM Central (14:14 UTC)
  workflow_dispatch:

jobs:
  run-fantasy-scripts:
    runs-on: ubuntu-latest

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: us-east-2
      S3_BUCKET_NAME: fantasy-sports-csvs

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests beautifulsoup4

      # ✅ Existing Scripts Here (Optional — keep or add your old ones)

      # ✅ NEW: Run DraftKings props script
      - name: Run DraftKings player props
        run: python "Fantasy Baseball/Player Projections/mlb_draftkings_player_props.py"

      # ✅ NEW: Run ESPN player projections script
      - name: Run ESPN player projections
        run: python "Fantasy Baseball/Player Projections/mlb_espn_player_projections.py"

      # ✅ NEW: Run Rotogrinders DraftKings projections script
      - name: Run Rotogrinders DraftKings projections
        run: python "Fantasy Baseball/Player Projections/mlb_rotogrinders_draftkings_projections.py"

      # ✅ NEW: Run Rotogrinders FanDuel projections script
      - name: Run Rotogrinders FanDuel projections
        run: python "Fantasy Baseball/Player Projections/mlb_rotogrinders_fanduel_projections.py"

      # ✅ Upload all Player Projections CSVs to S3
      - name: Upload projections to S3
        run: |
          pip install awscli
          aws s3 cp "Fantasy Baseball/Player Projections/" "s3://${{ env.S3_BUCKET_NAME }}/baseball/playerprojections/" --recursive --exclude "*" --include "*.csv" --acl public-read
