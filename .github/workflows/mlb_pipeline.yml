name: MLB Full Daily Pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 15 * * *"  # 8 AM PT daily

jobs:
  run-full-mlb-pipeline:
    runs-on: ubuntu-latest

    env:
      AWS_REGION: us-east-2
      BUCKET: fantasy-sports-csvs
      BASE: baseball

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install boto3 pytz

      - name: Upload raw source files to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          for dir in betting boxscores news probablestarters rosters weather; do
            if [ -d "$BASE/$dir" ]; then
              aws s3 cp --recursive "$BASE/$dir" "s3://$BUCKET/$BASE/$dir" --region $AWS_REGION
            fi
          done

      - name: Run combine.py to build structured file
        run: python combine.py

      - name: Run player_stats_analyzer.py to generate enhanced file
        run: python player_stats_analyzer.py

      - name: Upload enhanced structured file to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          ENHANCED_FILE=$(ls baseball/combined/enhanced_structured_players_*.json | tail -n 1)
          aws s3 cp "$ENHANCED_FILE" "s3://$BUCKET/$BASE/combined/$(basename $ENHANCED_FILE)" --region $AWS_REGION
